# HTTP 완벽 가이드

## 4월 7일

### 9장 웹 로봇

#### 개요
- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
- 많은 로봇이 웹 사이트에서 다른 웹사이트로 떠돌아 다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.
- 이러한 종류의 로봇들은 마치 스스로 마음을 가지고 있는 것처럼 자동으로 웹 사이트들을 탐색하며, 그 방식에 따라 크롤러, 스파이더, 웜, 봇 등 각양각색의 이름으로 불린다.
- 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
- 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇. 이것들은 웹을 떠돌면서 페이지의 개수르 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다.
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
- 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹페이지를 수집하는 가격 비교 로봇

#### 크롤러와 크롤링
- 웹 크롤러는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가르키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는 이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.
- 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어지는데, 이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

#### 어디에서 시작하는가 - 루트 집합
- 우선 출발지점을 주어야한다.
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 불린다.
- 일반적으로 좋은 루트 집합은 크고 인기 있는 웹사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

#### 링크 추출과 상대 링크 정상화
- 크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다.
- 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다.
- 크롤러가 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라, 이 목록은 보통 급속히 확장된다.

#### 순환 피하기
- 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다.
- 순환은 로봇을 함정에 빠드려서 멈추게 하거나 진행을 느려지게 한다.

#### 루프와 중복
- 크롤러가 빙빙 돌거나 시간을 허비하면 네트워크 대역폭을 다차지할 수 있다.
- 또한 웹서버의 불 필요한 부담을 주어 실제 사용자도 사이트에 접근할 수 없도록 막아버리게 될 수도 있다.
- 같은 콘텐츠를 크롤링하여 중복된 콘텐츠로 넘쳐나게 될 것이다.

#### 빵 부스러기의 흔적
- 수억 개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다. 
- URL 목록의 완벽한 검색은 불가능하다. 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.
- 크롤러가 방문한 곳을 관리하기 위해 사용하는 유용한 기법
    - 트리와 해시 테이블
    - 느슨한 존재 비트맵(공간 사용을 최소하기 위해, 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다.)
    - 체크 포인트(갑작스럽게 중단될 경우를 대비)
    - 파티셔닝(크롤러마다 부분적으로 담당파트를 정해서 크롤링)

#### 별칭과 로봇 순환
- 올바른 자료 구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는 게 쉽지 않을 때도 있다.
- 한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있다.
- p.253 표(같은 문서를 가르키는 다른 URL들)

#### URL 정규화하기
- 대부분의 웹 로봇은 URL들을 표준 형식으로 정규화 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려고 시도한다.
- URl을 정규화된 형식으로 변환
    - 1. 포트 번호가 명시되지 않았다면, 호스트 명에 :80을 추가한다.
    - 2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
    - 3. 샵(프래그먼트) 태그들을 제거한다.
- URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만, 로봇들은 URL을 표준 형식으로 변환하는 것만으로 제거할 수 없는 다른 URL 별칭도 만나게 될 것이다.

#### 파일 시스템 링크 순환
- 파일 시스템의 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있다.
- 이상한 낌새를 눈치 채지 못한 로봇은 루프로 빠져들 위험이 있다.
- 어떤 식으로든 루프 발견을 하지 못한다면, URL의 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환은 계속될 것이다.

#### 동적 가상 웹 공간
- 평범한 파일처럼 보이지만 사실은 게이트웨이 어플리케이션 URL을 만드는 것은 쉬운 일이다.
- 악덕한 서버는 가상의 URL로 요청을 받으면 새로운 가상 URL을 갖고 있는 새 HTML 페이지를 날조하여 만들 수 있다.
- 악의적인 웹 서버는 불쌍한 로봇을 무한한 가상 웹 공간 너머에 있는 이상한 나라의 앨리스의 세계로 여행을 보내버린다.
- 이보다 더 흔하게 일어날 수 있는 일은, 웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만드는 것이다.

#### 루프와 중복 피하기
- 모든 순환을 피하는 완벽한 방법은 없다.
- 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
- 웹에서 로봇이 올바르게 동작하기 위해 사용하는 기법들
    - 1. URL 정규화
        - URL을 표준 형태로 변환함
    - 2. 너비 우선 크롤링
        - 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최소화 할 수 있다.
    - 3. 스로틀링
        - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.
    - 4. URL 크기 제한
        - 로봇은 일정 길이를 넘는 URL의 크롤링을 거부할 수 있다.
    - 5. URL/사이트 블랙리스트
        - 문제를 일으키는 사이트나 URL이 발견될때마다 블랙리스트에 추가한다.
    - 6. 패턴 발견
        - 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다.
    - 7. 콘텐츠 지문
        - 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산하여 가지고 있는다. 
        - 다른 페이지에서 같은 체크섬이 존재하는지에 따라 해당 페이즈의 크롤링을 여부를 정한다.
    - 8. 사람의 모니터링
        - 로봇은 결국 자신에게 적용된 어떤 기법으로도 해결할 수 없는 문제에 봉찯하게 될 것이다. 사람이 모니터링을 통하여 해결해야 한다.

#### 로봇의 HTTP
- 로봇들은 HTTP 명세의 규칙을 지켜야 한다.
- HTTP 요청을 만들고 스스로를 HTTP/1.1 클라이언트라고 광고하는 로봇은 적절한 HTTP 요청 헤더를 사용해야 한다.

#### 요청 헤더 식별하기
- User-Agent
    - 서버에게 요청을 만든 로봇의 이름을 말해준다.
- From
    - 로봇의 사용자/관리자의 이메일 주소를 제공한다.
- Accept
    - 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
    - 이 헤더는 로봇이 관심 있는 유형의 콘텐츠만 받게 될 것임을 확신하는데 도움을 준다.
- Referer
    - 현재의 요청 URL을 포함한 문서의 URL을 제공한다.

#### 가상 호스팅
- 두개의 Host를 가진 웹서버에 요청시 Host 헤더는 필수

#### 조건부 요청
- 수십억 개의 웹페이지를 다운받게 될 수도 있는 인터넷 검색엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미가 있다.
- 이들 로봇 중의 몇몇은 시간이나 엔터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.

#### 응답 미루기
- HTTP의 특정 몇몇 기능을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알필요가 있다.
- 상태 코드
    - 일반적으로, 로봇들은 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.
- 엔터티
    - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.

#### User-Agent 타기팅
- 웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명시하고, 그 로봇들로부터의 요청을 예상해야 한다.
- 사이트 관리자들은 최소한 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 한다.

#### 부적절하게 동작하는 로봇들
- 폭주하는 로봇
    - 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있다.
    - 만약 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안결 줄수 있다.
- 오래된 URL
    - 몇몇 로봇은 URL의 목록을 방문한다. 그 목록은 오래되었을 수 있다.
    - 만약 웹 사이트가 그들의 콘텐츠를 많이 바꾸었다면, 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
- 길고 잘못된 URL
    - 순환이나 프로그래밍상의 오류로 인해 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청할 수 있다.
- 호기심이 지나친 로봇
    - 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있다.
- 동적 게이트웨이 접근
    - 로봇들이 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아니다.

#### 로봇 차단기
- robots.txt
    - 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.

#### 로봇 차단 표준
- 로봇 차단 표준은 임시방편으로 마련된 표준이다.
- 이 표준이 작성되고 있을 때, 이 표준을 소유하고 있는 주체가 없었고 업체들은 이 표준의 부분집합을 제각각 구현하고 있었다.
- 여전히 웹 사이트에 대한 로봇의 접근을 제어하는 능력은 불완전한데가 있지만 없는 것보다는 훨씬 낫고 대부분의 주류 업체들과 검색엔진 크롤러들은 이 차단 표준을 지원한다.

#### 웹 사이트웨 robots.txt 파일들
- 웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다.
- 만약 웹 사이트가 가상 호스팅된다면, 다른 모든 파일이 그러하듯이 각각의 가상 docroot에 서로 다른 robots.txt가 있을 수 있다.
- robots.txt 가져오기
- 응답 코드

#### robots.txt 파일 포맷
- 어떤 경로 밑에 있느냐와 상관없이 특정 이름의 디렉터리에 대해서는 크롤링을 못하게 하고 싶은 경우가 있을 수 있는데, robots.txt는 이를 표현할 수단을 제공해주지 않는다.
<pre><code>
User-Agent: Slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
</code></pre>
- p.270 표 참조

#### robots.txt의 캐싱과 만료
- 매 파일 접근마다 로봇이 robots.txt 파일을 새로 가져와야 했다면, 이는 로봇을 덜 효율적으로 만들 뿐 아니라 웹 서버의 부하도 두 배로 늘렸을 것이다.
- 대신 로봇은 주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 한다.
- 로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다.

#### HTML 로봇 제어 META 태그
- 로봇 META 지시자
- NOINDEX
- NOFOLLOW
- INDEX
- FOLLOW
- NOARCHIVE
- ALL
- NONE
- 검색엔진 META 태그

#### 로봇 에티켓
- p.277 ~ p.279

#### 검색 엔진
- 웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다.
- 인터넷 검색 엔진은 사용자가 전 세계의 어떤 주제에 대한 문서라도 찾을 수 있게 해 준다.
- 오늘날 가장 유명한 웹 사이트들의 상당수가 검색 엔진이다. 이들은 많은 웹 사용자들의 시작점인 동시에 사용자들이 관심 있는 정보를 찾을 수 있도록 도와주는 매우 유용한 서비스를 제공한다.
- 웹 크롤러들은 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다 주어서, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.

#### 넓게 생각하라
- 웹에서 수십억 개의 페이지들이 접근 가능한 오늘날, 인터넷 사용자들의 정보 찾기를 도와주는 검색엔진들은 필수가 되었다.

#### 현대적인 검색엔진의 아키텍처
- 오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대해 풀 텍스트 색인이라고 하는 복잡한 로컬 데이터베이스를 생성한다.
- 이 색인 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작한다.
- 풀 텍스트 색인 기껏 해봐야 웹의 특정 순간에 대한 스냅숏에 불과하다.

#### 풀 텍스트 색인
- 풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다.

#### 질의 보내기

#### 검색 결과를 정렬하고 보여주기

#### 스푸핑
- 사용자들은 자신이 찾는 내용이 검색 결과의 최상위 몇 줄에서 보이지 않는다면 대개 불만족스러워 하므로, 웹 사이트를 찾을 때 검색 결과의 순서는 중요하다.
- 수많은 키워드들을 나열한 가짜 페이지를 만들거나, 더 나아가서는 검색 엔진의 관련도 알고리즘을 더 잘 속일 수 있는, 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션을 만들어 사용한다.

#### 정리
- 이번기회에 크롤러와 검색엔진의 관계를 알 수 있었다.
- 크롤러를 만들어 무작정 크롤링 하기보다는 어떻게 제대로 해야하는지에 대한 공부를 할 수 있었다.
- 크롤링에도 에티켓이 존재한다는 걸 알 수 있었다.
- 크롤러가 나의 웹사이트를 크롤링 하지 못하도록 막는 방법이 있다는걸 알게 되었다.
